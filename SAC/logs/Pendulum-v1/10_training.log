SAC Agent Configuration:
hidden_units: [128, 128]
conv_layers: None
log_std_interval: [-20, 2]
gamma: 0.99
load_best: False
replay_buffer_size: 100000
batch_size: 64
init_temperature: 0.2
actor_lr: 0.0003
critic_lr: 0.0003
temperature_lr: 0.0003
actor_update_freq: 2
critic_target_update_freq: 2
tau: 0.005

Train Configuration:
gradient_steps: 1
max_episodes: 400
warmup_steps: 10000
print_interval: 10
checkpoint_interval: 50
test_episodes: 25
save_replay_buffer: False

Environment Seed: 42

Starting SAC Training

No pre-trained model found, training from scratch.

Episode 10 - Reward: -1234.59 - Alpha: 0.20000 - Steps: 2001
Episode 20 - Reward: -1241.78 - Alpha: 0.20000 - Steps: 4001
Episode 30 - Reward: -1062.67 - Alpha: 0.20000 - Steps: 6001
Episode 40 - Reward: -1170.46 - Alpha: 0.20000 - Steps: 8001
Episode 50 - Reward: -1071.77 - Alpha: 0.20000 - Steps: 10001
Episode 60 - Reward: -1433.03 - Alpha: 0.15076 - Steps: 12001
Episode 70 - Reward: -1041.06 - Alpha: 0.11678 - Steps: 14001
Episode 80 - Reward: -971.96 - Alpha: 0.09185 - Steps: 16001
Episode 90 - Reward: -970.26 - Alpha: 0.07288 - Steps: 18001
Episode 100 - Reward: -863.38 - Alpha: 0.05817 - Steps: 20001
Episode 110 - Reward: -240.84 - Alpha: 0.04687 - Steps: 22001
Episode 120 - Reward: -243.12 - Alpha: 0.03826 - Steps: 24001
Episode 130 - Reward: -122.68 - Alpha: 0.03156 - Steps: 26001
Episode 140 - Reward: -120.55 - Alpha: 0.02626 - Steps: 28001
Episode 150 - Reward: -120.04 - Alpha: 0.02202 - Steps: 30001
Episode 160 - Reward: -118.77 - Alpha: 0.01841 - Steps: 32001
Episode 170 - Reward: -117.79 - Alpha: 0.01528 - Steps: 34001
Episode 180 - Reward: -118.85 - Alpha: 0.01260 - Steps: 36001
Episode 190 - Reward: -119.30 - Alpha: 0.01028 - Steps: 38001
Episode 200 - Reward: -118.18 - Alpha: 0.00839 - Steps: 40001
Episode 210 - Reward: -118.72 - Alpha: 0.00688 - Steps: 42001
Episode 220 - Reward: -118.42 - Alpha: 0.00561 - Steps: 44001
Episode 230 - Reward: -118.40 - Alpha: 0.00455 - Steps: 46001
Episode 240 - Reward: -118.82 - Alpha: 0.00373 - Steps: 48001
Episode 250 - Reward: -118.15 - Alpha: 0.00306 - Steps: 50001
Episode 260 - Reward: -118.22 - Alpha: 0.00254 - Steps: 52001
Episode 270 - Reward: -118.55 - Alpha: 0.00212 - Steps: 54001
Episode 280 - Reward: -119.75 - Alpha: 0.00177 - Steps: 56001
Episode 290 - Reward: -119.43 - Alpha: 0.00148 - Steps: 58001
Episode 300 - Reward: -118.08 - Alpha: 0.00125 - Steps: 60001
Episode 310 - Reward: -118.14 - Alpha: 0.00109 - Steps: 62001
Episode 320 - Reward: -119.00 - Alpha: 0.00096 - Steps: 64001
Episode 330 - Reward: -117.77 - Alpha: 0.00088 - Steps: 66001
Episode 340 - Reward: -117.53 - Alpha: 0.00083 - Steps: 68001
Episode 350 - Reward: -117.80 - Alpha: 0.00077 - Steps: 70001
Episode 360 - Reward: -117.46 - Alpha: 0.00076 - Steps: 72001
Episode 370 - Reward: -117.49 - Alpha: 0.00075 - Steps: 74001
Episode 380 - Reward: -117.66 - Alpha: 0.00069 - Steps: 76001
Episode 390 - Reward: -116.99 - Alpha: 0.00070 - Steps: 78001
Episode 400 - Reward: -116.89 - Alpha: 0.00064 - Steps: 80001
Training time: 16m 42s
Reward over 25 episodes: -118.83 Â± 0.14
Total Reward of Policy Evaluation: -1751.86
Loaded model with reward: -116.89 and alpha: 0.00064
Loaded best agent checkpoint.
Total Reward of Policy Evaluation: -124.62
