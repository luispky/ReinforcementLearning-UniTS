SAC Agent Configuration:
hidden_units: [128, 128]
conv_layers: None
log_std_interval: [-20, 2]
gamma: 0.99
load_best: False
replay_buffer_size: 100000
batch_size: 64
init_temperature: 0.2
actor_lr: 0.0003
critic_lr: 0.0003
temperature_lr: 0.0003
actor_update_freq: 2
critic_target_update_freq: 2
tau: 0.005

Train Configuration:
gradient_steps: 1
max_episodes: 400
warmup_steps: 10000
print_interval: 10
checkpoint_interval: 50
test_episodes: 25
save_replay_buffer: False

Environment Seed: None

Starting SAC Training

No pre-trained model found, training from scratch.

Episode 10 - Reward: -1198.19 - Alpha: 0.20000 - Steps: 2001
Episode 20 - Reward: -1482.38 - Alpha: 0.20000 - Steps: 4001
Episode 30 - Reward: -1177.04 - Alpha: 0.20000 - Steps: 6001
Episode 40 - Reward: -1195.95 - Alpha: 0.20000 - Steps: 8001
Episode 50 - Reward: -1175.71 - Alpha: 0.20000 - Steps: 10001
Episode 60 - Reward: -1625.79 - Alpha: 0.15644 - Steps: 12001
Episode 70 - Reward: -1276.09 - Alpha: 0.13356 - Steps: 14001
Episode 80 - Reward: -663.55 - Alpha: 0.12939 - Steps: 16001
Episode 90 - Reward: -1383.58 - Alpha: 0.14363 - Steps: 18001
Episode 100 - Reward: -0.45 - Alpha: 0.17324 - Steps: 20001
Episode 110 - Reward: -244.39 - Alpha: 0.17481 - Steps: 22001
Episode 120 - Reward: -123.57 - Alpha: 0.17287 - Steps: 24001
Episode 130 - Reward: -1.14 - Alpha: 0.16555 - Steps: 26001
Episode 140 - Reward: -125.41 - Alpha: 0.17934 - Steps: 28001
Episode 150 - Reward: -1.27 - Alpha: 0.19818 - Steps: 30001
Episode 160 - Reward: -120.41 - Alpha: 0.21250 - Steps: 32001
Episode 170 - Reward: -240.74 - Alpha: 0.22416 - Steps: 34001
Episode 180 - Reward: -121.80 - Alpha: 0.21670 - Steps: 36001
Episode 190 - Reward: -124.17 - Alpha: 0.19984 - Steps: 38001
Episode 200 - Reward: -246.42 - Alpha: 0.17986 - Steps: 40001
Episode 210 - Reward: -232.26 - Alpha: 0.16173 - Steps: 42001
Episode 220 - Reward: -119.43 - Alpha: 0.15118 - Steps: 44001
Episode 230 - Reward: -127.39 - Alpha: 0.14007 - Steps: 46001
Episode 240 - Reward: -128.05 - Alpha: 0.13987 - Steps: 48001
Episode 250 - Reward: -1.20 - Alpha: 0.13734 - Steps: 50001
Episode 260 - Reward: -125.99 - Alpha: 0.13720 - Steps: 52001
Episode 270 - Reward: -119.08 - Alpha: 0.14127 - Steps: 54001
Episode 280 - Reward: -226.99 - Alpha: 0.13775 - Steps: 56001
Episode 290 - Reward: -119.65 - Alpha: 0.13272 - Steps: 58001
Episode 300 - Reward: -244.84 - Alpha: 0.12559 - Steps: 60001
Episode 310 - Reward: -114.41 - Alpha: 0.11515 - Steps: 62001
Episode 320 - Reward: -120.75 - Alpha: 0.10416 - Steps: 64001
Episode 330 - Reward: -1.39 - Alpha: 0.09620 - Steps: 66001
Episode 340 - Reward: -120.73 - Alpha: 0.08812 - Steps: 68001
Episode 350 - Reward: -342.10 - Alpha: 0.08216 - Steps: 70001
Episode 360 - Reward: -115.18 - Alpha: 0.07688 - Steps: 72001
Episode 370 - Reward: -115.18 - Alpha: 0.06972 - Steps: 74001
Episode 380 - Reward: -2.29 - Alpha: 0.06474 - Steps: 76001
Episode 390 - Reward: -116.56 - Alpha: 0.05887 - Steps: 78001
Episode 400 - Reward: -120.65 - Alpha: 0.05484 - Steps: 80001
Training time: 16m 6s
Reward over 25 episodes: -118.34 Â± 0.11
Total Reward of Policy Evaluation: -120.76
Loaded model with reward: -0.45 and alpha: 0.17324
Loaded best agent checkpoint.
Total Reward of Policy Evaluation: -1254.70
