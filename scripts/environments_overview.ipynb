{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring Pendulum-v1 environment...\n",
      "\n",
      "Observation Space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "Observation Space Shape: (3,)\n",
      "Action Space: Box(-2.0, 2.0, (1,), float32)\n",
      "Action Space Shape: (1,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 200\n",
      "Action Sample:\n",
      " [0.8790874]\n",
      "State Sample:\n",
      " [ 0.33019865  0.94391143 -0.9206078 ]\n",
      "Total Reward: -966.87\n",
      "\n",
      "Exploring BipedalWalker-v3 environment...\n",
      "\n",
      "Observation Space: Box([-3.1415927 -5.        -5.        -5.        -3.1415927 -5.\n",
      " -3.1415927 -5.        -0.        -3.1415927 -5.        -3.1415927\n",
      " -5.        -0.        -1.        -1.        -1.        -1.\n",
      " -1.        -1.        -1.        -1.        -1.        -1.       ], [3.1415927 5.        5.        5.        3.1415927 5.        3.1415927\n",
      " 5.        5.        3.1415927 5.        3.1415927 5.        5.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.       ], (24,), float32)\n",
      "Observation Space Shape: (24,)\n",
      "Action Space: Box(-1.0, 1.0, (4,), float32)\n",
      "Action Space Shape: (4,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1600\n",
      "Action Sample:\n",
      " [-0.84259367 -0.7720526   0.81331235 -0.43600985]\n",
      "State Sample:\n",
      " [ 2.7475618e-03 -1.8054620e-06  1.4042783e-04 -1.5999988e-02\n",
      "  9.2126191e-02 -1.8531000e-04  8.6017299e-01  1.5402581e-03\n",
      "  1.0000000e+00  3.2531779e-02 -1.8530151e-04  8.5372204e-01\n",
      "  1.3105714e-04  1.0000000e+00  4.4081393e-01  4.4582003e-01\n",
      "  4.6142268e-01  4.8955008e-01  5.3410268e-01  6.0246092e-01\n",
      "  7.0914876e-01  8.8593167e-01  1.0000000e+00  1.0000000e+00]\n",
      "Reward at termination -100.00\n",
      "Total Reward: -98.59\n",
      "\n",
      "Exploring HalfCheetah-v4 environment...\n",
      "\n",
      "Observation Space: Box(-inf, inf, (17,), float64)\n",
      "Observation Space Shape: (17,)\n",
      "Action Space: Box(-1.0, 1.0, (6,), float32)\n",
      "Action Space Shape: (6,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1000\n",
      "Action Sample:\n",
      " [ 0.83296776  0.47575125  0.5279191  -0.20775631  0.39896318  0.52961564]\n",
      "State Sample:\n",
      " [ 0.06737798 -0.08950081  0.06511234 -0.02231878 -0.09477267  0.0135048\n",
      "  0.0627721   0.00435279 -0.09676124 -0.13671892 -0.06987868  0.11095225\n",
      " -0.10369244  0.16591906 -0.05736309  0.11103176  0.07587182]\n",
      "Total Reward: -439.86\n",
      "\n",
      "Exploring Humanoid-v4 environment...\n",
      "\n",
      "Observation Space: Box(-inf, inf, (376,), float64)\n",
      "Observation Space Shape: (376,)\n",
      "Action Space: Box(-0.4, 0.4, (17,), float32)\n",
      "Action Space Shape: (17,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1000\n",
      "Action Sample:\n",
      " [-0.26835853 -0.2465694   0.08314324 -0.25594065  0.33802867  0.23555729\n",
      " -0.1518709  -0.16516505 -0.07053959  0.11282156  0.10265356 -0.10910724\n",
      "  0.2713159   0.31156394  0.22428685 -0.370974    0.30642954]\n",
      "State Sample:\n",
      " [ 1.39595814e+00  1.00859233e+00 -3.62339551e-03  4.46471538e-03\n",
      "  4.89219628e-03  3.40404868e-03  3.79215301e-04  9.72046831e-03\n",
      " -3.90413768e-03  2.17481063e-03  1.46315325e-03 -9.21262810e-03\n",
      " -5.94846177e-03  5.24722265e-04  6.36494760e-03  3.04863347e-03\n",
      " -3.82135960e-03 -2.01678729e-04  6.55514790e-03 -8.68656741e-03\n",
      " -6.36457222e-03  7.18591855e-03 -6.75453123e-03  7.84485371e-04\n",
      " -9.18216579e-03 -8.23451580e-03  2.20750406e-03 -8.74625170e-03\n",
      " -3.69803221e-03 -1.57259908e-03  2.24110651e-03 -4.66980012e-03\n",
      " -2.22353798e-03 -1.39073601e-04  6.94432542e-03 -8.70663021e-03\n",
      "  6.28866622e-03  4.17169485e-03  3.06887848e-03  2.89252500e-03\n",
      "  8.03687097e-03 -6.21018395e-03 -1.14989426e-03  3.06149918e-03\n",
      "  1.43395012e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  2.30371828e+00\n",
      "  2.28595750e+00  4.26144682e-02  2.53357162e-04  4.82997547e-02\n",
      " -4.91753338e-03 -1.08289641e-01  8.31582872e-03  4.35643836e+00\n",
      "  8.90746237e+00  9.54259657e-02  9.04363000e-02  1.09751827e-02\n",
      "  1.63993393e-05  9.78396654e-03  6.01620025e-04 -5.03060117e-02\n",
      " -2.86945716e-03  4.39949484e-01  2.26194671e+00  5.81029862e-02\n",
      "  4.23921117e-02  6.46332615e-02 -1.30649608e-04  8.42352074e-03\n",
      "  2.68508589e-04 -2.85215844e-01 -1.15528517e-02  1.95379537e-01\n",
      "  6.61619413e+00  2.74553698e-01  2.32542205e-01  5.50057630e-02\n",
      " -1.02729750e-02 -1.94464563e-02 -8.11028488e-02 -1.06405815e-01\n",
      " -4.59025203e-01 -8.58928566e-01  4.75175093e+00  9.31810468e-01\n",
      "  9.10152153e-01  2.83243447e-02 -6.36968143e-03 -3.93199408e-02\n",
      " -1.43189661e-01 -6.91232141e-02 -2.53915631e-01 -1.55353377e+00\n",
      "  2.75569617e+00  1.04984832e+00  1.03608165e+00  2.04103962e-02\n",
      " -4.48791924e-03 -3.70792317e-02 -1.24760416e-01 -4.85496019e-02\n",
      " -1.63354747e-01 -1.34963848e+00  1.76714587e+00  2.72288983e-01\n",
      "  2.32483082e-01  5.43339596e-02  1.16119101e-02 -2.25361608e-02\n",
      "  8.00179512e-02 -1.22279069e-01  4.51639266e-01 -8.56584581e-01\n",
      "  4.75175093e+00  9.30953186e-01  9.09149658e-01  2.93550731e-02\n",
      "  7.23204268e-03 -4.37996067e-02  1.45057020e-01 -7.75105913e-02\n",
      "  2.57075399e-01 -1.55225010e+00  2.75569617e+00  1.04944816e+00\n",
      "  1.03499800e+00  2.14132141e-02  4.87971889e-03 -3.92121034e-02\n",
      "  1.28118844e-01 -5.13732648e-02  1.67853359e-01 -1.34882427e+00\n",
      "  1.76714587e+00  4.29566414e-01  3.35223751e-01  1.22757710e-01\n",
      "  3.47986112e-02 -4.74872379e-02  1.75416215e-01  1.20112227e-01\n",
      " -4.13655622e-01  7.23959854e-01  1.66108048e+00  3.21264241e-01\n",
      "  3.48851401e-01  1.75584029e-01  7.74951746e-02 -1.57698675e-01\n",
      "  1.25153482e-01  3.45371556e-01 -2.91562098e-01  5.46291475e-01\n",
      "  1.22954019e+00  4.24189989e-01  3.28037244e-01  1.22443281e-01\n",
      " -3.30910361e-02 -4.39282626e-02 -1.74313089e-01  1.13257285e-01\n",
      "  4.15446319e-01  7.16615495e-01  1.66108048e+00  3.18544771e-01\n",
      "  3.41757588e-01  1.74478383e-01 -7.75198746e-02 -1.54168838e-01\n",
      " -1.25747077e-01  3.40311984e-01  2.95605437e-01  5.41125579e-01\n",
      "  1.22954019e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -8.33241431e-03\n",
      "  2.06451188e-03 -8.68826254e-03 -7.69878188e-03 -3.08793710e-03\n",
      " -9.19675726e-03 -8.32957476e-03  4.65503776e-04 -1.23749514e-02\n",
      " -7.28088981e-03 -3.16794722e-03 -9.16173303e-03 -6.08866322e-03\n",
      "  4.94923240e-04 -1.23778459e-02 -7.28469471e-03 -2.87780616e-03\n",
      " -9.15850665e-03 -1.07587621e-02  2.91617083e-04 -1.45951673e-02\n",
      " -7.06210795e-03 -2.87603163e-03 -9.62747924e-03 -1.06524897e-02\n",
      " -6.65188919e-03 -1.45860983e-02 -9.79705780e-03 -2.91766901e-03\n",
      " -9.45764870e-03 -1.06524897e-02 -6.65188919e-03 -1.45860983e-02\n",
      " -9.79705780e-03 -2.91766901e-03 -9.45764870e-03  2.55585727e-03\n",
      "  4.83404628e-03 -1.86418833e-02 -7.85798141e-03 -3.11960610e-03\n",
      " -1.01171517e-02  2.59449760e-03  1.76552307e-03 -1.86681008e-02\n",
      " -9.06722681e-03 -3.13551958e-03 -1.00368530e-02  2.59449760e-03\n",
      "  1.76552307e-03 -1.86681008e-02 -9.06722681e-03 -3.13551958e-03\n",
      " -1.00368530e-02 -5.88428767e-03 -2.34704602e-03 -1.79603295e-03\n",
      " -6.58788988e-03 -1.77232504e-03 -8.74925411e-03 -5.95203377e-03\n",
      "  1.99846029e-03 -6.23206385e-03 -6.49922421e-03 -1.03044905e-03\n",
      " -8.02387141e-03 -9.26683787e-03  4.71705267e-03 -7.01915680e-03\n",
      " -8.77437124e-03 -3.54901921e-03 -9.06615855e-03 -9.27296829e-03\n",
      "  3.68867229e-03 -8.01846113e-03 -8.78439917e-03 -3.38393088e-03\n",
      " -9.23598882e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "Reward at termination 5.61\n",
      "Total Reward: 124.51\n",
      "\n",
      "Exploring Ant-v4 environment...\n",
      "\n",
      "Observation Space: Box(-inf, inf, (27,), float64)\n",
      "Observation Space Shape: (27,)\n",
      "Action Space: Box(-1.0, 1.0, (8,), float32)\n",
      "Action Space Shape: (8,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1000\n",
      "Action Sample:\n",
      " [-0.35393023 -0.13354822  0.10452141 -0.53783727 -0.0106473   0.03827037\n",
      " -0.97928655  0.37305397]\n",
      "State Sample:\n",
      " [ 0.80143188  1.00968837 -0.02328637 -0.0735839  -0.02679333 -0.0206026\n",
      " -0.08978935 -0.03477991 -0.01514403  0.03403693 -0.07456264 -0.00170629\n",
      " -0.09395721 -0.13466401 -0.10903418  0.03859259  0.26543915  0.06832863\n",
      "  0.21686883  0.00235578  0.06202155 -0.06729455  0.152437   -0.14861494\n",
      "  0.04937453 -0.03964366 -0.05233917]\n",
      "Reward at termination -1.85\n",
      "Total Reward: -14.30\n",
      "\n",
      "Exploring Walker2d-v4 environment...\n",
      "\n",
      "Observation Space: Box(-inf, inf, (17,), float64)\n",
      "Observation Space Shape: (17,)\n",
      "Action Space: Box(-1.0, 1.0, (6,), float32)\n",
      "Action Space Shape: (6,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1000\n",
      "Action Sample:\n",
      " [ 0.46681437 -0.99139214  0.15511173 -0.8867847  -0.40384603 -0.9517336 ]\n",
      "State Sample:\n",
      " [ 1.24999328e+00  9.93931525e-05 -9.40929606e-04  4.08874787e-03\n",
      "  4.54348220e-03 -4.31288690e-03  4.99924009e-03 -3.81243252e-03\n",
      "  1.84146628e-03  3.92070157e-03 -4.57990691e-04 -2.39340358e-03\n",
      " -3.36519468e-03 -2.05330649e-04  1.15392331e-03  3.58216274e-03\n",
      "  4.29585326e-03]\n",
      "Reward at termination 0.16\n",
      "Total Reward: 5.30\n",
      "\n",
      "Exploring CarRacing-v2 environment...\n",
      "\n",
      "Observation Space: Box(0, 255, (96, 96, 3), uint8)\n",
      "Observation Space Shape: (96, 96, 3)\n",
      "Action Space: Box([-1.  0.  0.], 1.0, (3,), float32)\n",
      "Action Space Shape: (3,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1000\n",
      "Action Sample:\n",
      " [-0.5346737   0.49482986  0.36310053]\n",
      "State Sample:\n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "Total Reward: -32.43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "env_names = ['Pendulum-v1', 'BipedalWalker-v3',\n",
    "             'HalfCheetah-v4', 'Humanoid-v4',\n",
    "             'Ant-v4', 'Walker2d-v4',\n",
    "             'CarRacing-v2']\n",
    "\n",
    "# Directory to save the GIFs\n",
    "overview = 'overview'\n",
    "os.makedirs(overview, exist_ok=True)\n",
    "\n",
    "for env_name in env_names:\n",
    "\n",
    "    print(f\"Exploring {env_name} environment...\\n\")\n",
    "\n",
    "    gif_path = os.path.join(overview, f'{env_name}.gif')\n",
    "    \n",
    "    if not os.path.exists(gif_path):\n",
    "\n",
    "        # Create the environment\n",
    "        env = gym.make(env_name, render_mode='rgb_array')\n",
    "\n",
    "        # Initialize variables\n",
    "        frames = []\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        # Print observation space and shape\n",
    "        print(f\"Observation Space: {env.observation_space}\")\n",
    "        print(f\"Observation Space Shape: {env.observation_space.shape}\")\n",
    "        \n",
    "        # Print action space and shape\n",
    "        print(f\"Action Space: {env.action_space}\")\n",
    "        print(f\"Action Space Shape: {env.action_space.shape}\")\n",
    "        \n",
    "        # Print reward range\n",
    "        print(f\"Reward Range: {env.reward_range}\")\n",
    "        \n",
    "        # Print max episode steps\n",
    "        print(f\"Max Episode Steps: {env.spec.max_episode_steps}\")    \n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        print(f\"Action Sample:\\n {action}\")\n",
    "        print(f\"State Sample:\\n {state}\")\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Capture the frame\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(f\"Reward at termination: {reward:.2f}\")\n",
    "            \n",
    "            if truncated:\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        # Save the frames as a GIF\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "        print(f\"Total Reward: {total_reward:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring Pendulum-v1 environment...\n",
      "\n",
      "Observation Space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "Observation Space Shape: (3,)\n",
      "Action Space: Box(-1.0, 1.0, (1,), float32)\n",
      "Action Space Shape: (1,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 200\n",
      "Action Sample:\n",
      " [-0.91727823]\n",
      "Total Reward: -867.86\n",
      "\n",
      "Exploring BipedalWalker-v3 environment...\n",
      "\n",
      "Observation Space: Box([-3.1415927 -5.        -5.        -5.        -3.1415927 -5.\n",
      " -3.1415927 -5.        -0.        -3.1415927 -5.        -3.1415927\n",
      " -5.        -0.        -1.        -1.        -1.        -1.\n",
      " -1.        -1.        -1.        -1.        -1.        -1.       ], [3.1415927 5.        5.        5.        3.1415927 5.        3.1415927\n",
      " 5.        5.        3.1415927 5.        3.1415927 5.        5.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.       ], (24,), float32)\n",
      "Observation Space Shape: (24,)\n",
      "Action Space: Box(-1.0, 1.0, (4,), float32)\n",
      "Action Space Shape: (4,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1600\n",
      "Action Sample:\n",
      " [-0.8032901  -0.09090377 -0.38564473  0.24655226]\n",
      "Reward at termination: -100.00\n",
      "Total Reward: -111.02\n",
      "\n",
      "Exploring HalfCheetah-v4 environment...\n",
      "\n",
      "Observation Space: Box(-inf, inf, (17,), float64)\n",
      "Observation Space Shape: (17,)\n",
      "Action Space: Box(-1.0, 1.0, (6,), float32)\n",
      "Action Space Shape: (6,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1000\n",
      "Action Sample:\n",
      " [ 0.00685629 -0.98485506  0.87351274  0.5552962  -0.7614511   0.8688135 ]\n",
      "Total Reward: -168.53\n",
      "\n",
      "Exploring Humanoid-v4 environment...\n",
      "\n",
      "Observation Space: Box(-inf, inf, (376,), float64)\n",
      "Observation Space Shape: (376,)\n",
      "Action Space: Box(-1.0, 1.0, (17,), float32)\n",
      "Action Space Shape: (17,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1000\n",
      "Action Sample:\n",
      " [ 0.7381483  -0.46729416 -0.8796747  -0.03288985  0.85300153 -0.3326184\n",
      "  0.32820284 -0.7189107  -0.7577302  -0.06129505 -0.5957312  -0.47298735\n",
      " -0.53250784  0.6095357   0.3536085  -0.620326    0.78281456]\n",
      "Reward at termination: 5.35\n",
      "Total Reward: 180.15\n",
      "\n",
      "Exploring Ant-v4 environment...\n",
      "\n",
      "Observation Space: Box(-inf, inf, (27,), float64)\n",
      "Observation Space Shape: (27,)\n",
      "Action Space: Box(-1.0, 1.0, (8,), float32)\n",
      "Action Space Shape: (8,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1000\n",
      "Action Sample:\n",
      " [ 0.38153404  0.49138948  0.87327075  0.529519    0.5754741  -0.4240717\n",
      "  0.11996184 -0.7624046 ]\n",
      "Reward at termination: -2.17\n",
      "Total Reward: 3.49\n",
      "\n",
      "Exploring Walker2d-v4 environment...\n",
      "\n",
      "Observation Space: Box(-inf, inf, (17,), float64)\n",
      "Observation Space Shape: (17,)\n",
      "Action Space: Box(-1.0, 1.0, (6,), float32)\n",
      "Action Space Shape: (6,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1000\n",
      "Action Sample:\n",
      " [ 0.47814414 -0.5316715   0.5850922   0.47237223 -0.81052667 -0.7789383 ]\n",
      "Reward at termination: -0.18\n",
      "Total Reward: 4.02\n",
      "\n",
      "Exploring CarRacing-v2 environment...\n",
      "\n",
      "Observation Space: Box(0, 255, (96, 96, 3), uint8)\n",
      "Observation Space Shape: (96, 96, 3)\n",
      "Action Space: Box(-1.0, 1.0, (3,), float32)\n",
      "Action Space Shape: (3,)\n",
      "Reward Range: (-inf, inf)\n",
      "Max Episode Steps: 1000\n",
      "Action Sample:\n",
      " [-0.32049584 -0.8117612  -0.33549738]\n",
      "Total Reward: -31.03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "\n",
    "env_names = ['Pendulum-v1', 'BipedalWalker-v3',\n",
    "             'HalfCheetah-v4', 'Humanoid-v4',\n",
    "             'Ant-v4', 'Walker2d-v4',\n",
    "             'CarRacing-v2']\n",
    "\n",
    "for env_name in env_names:\n",
    "\n",
    "    print(f\"Exploring {env_name} environment...\\n\")\n",
    "\n",
    "    # Create the environment\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    env = RescaleAction(env, min_action=-1, max_action=1)\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    # Print observation space and shape\n",
    "    print(f\"Observation Space: {env.observation_space}\")\n",
    "    print(f\"Observation Space Shape: {env.observation_space.shape}\")\n",
    "    \n",
    "    # Print action space and shape\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "    print(f\"Action Space Shape: {env.action_space.shape}\")\n",
    "    \n",
    "    # Print reward range\n",
    "    print(f\"Reward Range: {env.reward_range}\")\n",
    "    \n",
    "    # Print max episode steps\n",
    "    print(f\"Max Episode Steps: {env.spec.max_episode_steps}\")    \n",
    "\n",
    "    action = env.action_space.sample()\n",
    "    print(f\"Action Sample:\\n {action}\")\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(f\"Reward at termination: {reward:.2f}\")\n",
    "        \n",
    "        if truncated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(f\"Total Reward: {total_reward:.2f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
